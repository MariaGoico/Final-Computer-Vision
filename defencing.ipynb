{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9e8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.transforms import v2\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d29e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_dir):\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "\n",
    "        # Only list image files\n",
    "        self.image_files = sorted(\n",
    "            [f for f in os.listdir(images_dir) if f.lower().endswith(\".jpg\")]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        base_name = os.path.splitext(img_name)[0]\n",
    "\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        label_path = os.path.join(self.labels_dir, base_name + \".png\")\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = Image.open(label_path).convert(\"L\")  # binary mask\n",
    "\n",
    "        # To numpy\n",
    "        image = np.array(image, dtype=np.float32) / 255.0\n",
    "        label = np.array(label, dtype=np.float32) / 255.0\n",
    "\n",
    "        # To tensors\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        label = torch.from_numpy(label)\n",
    "\n",
    "        # Ensure binary mask\n",
    "        label = (label > 0.5).long()\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class SegmentationDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size=4, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = SegmentationDataset(\n",
    "            images_dir=os.path.join(self.data_dir, \"Training Set\", \"Training_Images\"),\n",
    "            labels_dir=os.path.join(self.data_dir, \"Training Set\", \"Training_Labels\"),\n",
    "        )\n",
    "\n",
    "        self.test_dataset = SegmentationDataset(\n",
    "            images_dir=os.path.join(self.data_dir, \"Test Set\", \"Test_Images\"),\n",
    "            labels_dir=os.path.join(self.data_dir, \"Test Set\", \"Test_Labels\"),\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_sample(dataset, idx=0):\n",
    "    image, mask = dataset[idx]\n",
    "\n",
    "    image = image.permute(1, 2, 0).numpy()\n",
    "    mask = mask.numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(mask, cmap=\"gray\")\n",
    "    axes[1].set_title(\"Binary Mask\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109e3671",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = SegmentationDataModule(data_dir=\"dataset\", batch_size=2)\n",
    "dm.setup()\n",
    "\n",
    "visualize_sample(dm.train_dataset, idx=0)\n",
    "visualize_sample(dm.test_dataset, idx=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9b226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModule(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-3, use_weighted_loss=True):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Creating model\n",
    "        self.model = smp.Unet(\n",
    "            encoder_name='resnet50',\n",
    "            encoder_weights='imagenet',\n",
    "            in_channels=3,\n",
    "            classes=3\n",
    "        )\n",
    "        self.training_step_outputs = defaultdict(float)\n",
    "        self.validation_step_outputs = defaultdict(float)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return {\"optimizer\": optimizer}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch['masked_image']\n",
    "        y = batch['original_image']\n",
    "        mask = batch['mask']\n",
    "        weights = batch['weights']\n",
    "        \n",
    "        y_hat = self(x)\n",
    "        \n",
    "        # Calcular loss con o sin pesos\n",
    "        if self.hparams.use_weighted_loss:\n",
    "            # Loss ponderado por los weights\n",
    "            loss_per_pixel = F.l1_loss(y_hat, y, reduction='none')  # (B, C, H, W)\n",
    "            # Expandir weights para que coincida con los canales de color\n",
    "            weights_expanded = weights.expand_as(loss_per_pixel)  # (B, C, H, W)\n",
    "            weighted_loss = loss_per_pixel * weights_expanded\n",
    "            loss = weighted_loss.sum() / (weights_expanded.sum() + 1e-8)\n",
    "        else:\n",
    "            # Loss simple solo en áreas enmascaradas\n",
    "            loss = (F.l1_loss(y_hat, y, reduction='none') * mask).sum() / (mask.sum() + 1e-8)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.training_step_outputs['loss'] += loss.detach().cpu()\n",
    "        self.training_step_outputs['steps'] += 1\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch['masked_image']\n",
    "        y = batch['original_image']\n",
    "        mask = batch['mask']\n",
    "        weights = batch['weights']\n",
    "        \n",
    "        y_hat = self(x)\n",
    "        \n",
    "        # Calcular loss con o sin pesos\n",
    "        if self.hparams.use_weighted_loss:\n",
    "            # Loss ponderado por los weights\n",
    "            loss_per_pixel = F.l1_loss(y_hat, y, reduction='none')  # (B, C, H, W)\n",
    "            # Expandir weights para que coincida con los canales de color\n",
    "            weights_expanded = weights.expand_as(loss_per_pixel)  # (B, C, H, W)\n",
    "            weighted_loss = loss_per_pixel * weights_expanded\n",
    "            loss = weighted_loss.sum() / (weights_expanded.sum() + 1e-8)\n",
    "        else:\n",
    "            # Loss simple solo en áreas enmascaradas\n",
    "            loss = (F.l1_loss(y_hat, y, reduction='none') * mask).sum() / (mask.sum() + 1e-8)\n",
    "        \n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.validation_step_outputs['loss'] += loss.detach().cpu()\n",
    "        self.validation_step_outputs['steps'] += 1\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self): \n",
    "        avg_loss = self.training_step_outputs['loss'] / self.training_step_outputs['steps']\n",
    "        print(f\"Average training loss for epoch {self.current_epoch}: {avg_loss.item():.4f}\")\n",
    "        self.training_step_outputs.clear() \n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = self.validation_step_outputs['loss'] / self.validation_step_outputs['steps']\n",
    "        print(f\"Average validation loss for epoch {self.current_epoch}: {avg_loss.item():.4f}\")\n",
    "        self.validation_step_outputs.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3be499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "class InpaintingInferenceDataset(Dataset):\n",
    "    \"\"\"Dataset para inferencia de inpainting usando máscaras de segmentación\"\"\"\n",
    "    def __init__(self, images_dir, labels_dir, mean, std):\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "        # Only list image files\n",
    "        self.image_files = sorted(\n",
    "            [f for f in os.listdir(images_dir) if f.lower().endswith(\".jpg\")]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        base_name = os.path.splitext(img_name)[0]\n",
    "        \n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        label_path = os.path.join(self.labels_dir, base_name + \".png\")\n",
    "        \n",
    "        # Load image and mask\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(label_path).convert(\"L\")  # binary mask\n",
    "        \n",
    "        # To numpy\n",
    "        image = np.array(image, dtype=np.float32) / 255.0\n",
    "        mask = np.array(mask, dtype=np.float32) / 255.0\n",
    "        \n",
    "        # To tensors\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)  # (3, H, W)\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0)  # (1, H, W)\n",
    "        \n",
    "        # Binary mask (1 where we want to inpaint, 0 otherwise)\n",
    "        mask = (mask > 0.5).float()\n",
    "        \n",
    "        # Create masked image (set masked regions to 0)\n",
    "        masked_image = image * (1 - mask)\n",
    "        \n",
    "        # Normalize masked image for model input\n",
    "        normalized_masked = masked_image.clone()\n",
    "        for i in range(3):\n",
    "            normalized_masked[i] = (normalized_masked[i] - self.mean[i]) / self.std[i]\n",
    "        \n",
    "        return {\n",
    "            'original_image': image,  # Original sin normalizar\n",
    "            'masked_image': normalized_masked,  # Normalizado para el modelo\n",
    "            'mask': mask,\n",
    "            'filename': img_name\n",
    "        }\n",
    "\n",
    "class InpaintingInferenceDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size=4, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        # Normalization parameters from training\n",
    "        self.mean = [0.34437724, 0.38029198, 0.40777111]\n",
    "        self.std = [0.20265734, 0.13689059, 0.11554374]\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        self.test_dataset = InpaintingInferenceDataset(\n",
    "            images_dir=os.path.join(self.data_dir, \"Test Set\", \"Test_Images\"),\n",
    "            labels_dir=os.path.join(self.data_dir, \"Test Set\", \"Test_Labels\"),\n",
    "            mean=self.mean,\n",
    "            std=self.std\n",
    "        )\n",
    "        \n",
    "        self.train_dataset = InpaintingInferenceDataset(\n",
    "            images_dir=os.path.join(self.data_dir, \"Training Set\", \"Training_Images\"),\n",
    "            labels_dir=os.path.join(self.data_dir, \"Training Set\", \"Training_Labels\"),\n",
    "            mean=self.mean,\n",
    "            std=self.std\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "def denormalize(img, mean, std):\n",
    "    \"\"\"Denormalize image for visualization\"\"\"\n",
    "    img = img.clone()\n",
    "    for i in range(3):\n",
    "        img[i] = img[i] * std[i] + mean[i]\n",
    "    return img.clamp(0, 1)\n",
    "\n",
    "def visualize_inpainting_segmentation(model, dataloader, device='cuda', num_images=4, save_dir='inpainting_results'):\n",
    "    \"\"\"\n",
    "    Visualize inpainting results on segmentation dataset\n",
    "    \n",
    "    Args:\n",
    "        model: Trained inpainting model\n",
    "        dataloader: DataLoader with inference data\n",
    "        device: 'cuda' or 'cpu'\n",
    "        num_images: Number of images to visualize\n",
    "        save_dir: Directory to save individual results\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Create save directory\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Normalization parameters\n",
    "    mean = [0.34437724, 0.38029198, 0.40777111]\n",
    "    std = [0.20265734, 0.13689059, 0.11554374]\n",
    "    \n",
    "    # Get a batch\n",
    "    batch = next(iter(dataloader))\n",
    "    original_images = batch['original_image'][:num_images]\n",
    "    masked_images_norm = batch['masked_image'][:num_images].to(device)\n",
    "    masks = batch['mask'][:num_images].to(device)\n",
    "    filenames = batch['filename'][:num_images]\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        predicted_images = model(masked_images_norm)\n",
    "    \n",
    "    # Denormalize predictions\n",
    "    predicted_images_denorm = []\n",
    "    for i in range(len(predicted_images)):\n",
    "        pred_denorm = denormalize(predicted_images[i].cpu(), mean, std)\n",
    "        predicted_images_denorm.append(pred_denorm)\n",
    "    predicted_images_denorm = torch.stack(predicted_images_denorm)\n",
    "    \n",
    "    # Create composed images: original + predicted in masked areas\n",
    "    composed_images = []\n",
    "    for i in range(len(original_images)):\n",
    "        composed = original_images[i] * (1 - masks[i].cpu()) + predicted_images_denorm[i] * masks[i].cpu()\n",
    "        composed_images.append(composed)\n",
    "    composed_images = torch.stack(composed_images)\n",
    "    \n",
    "    # Move to CPU\n",
    "    masks = masks.cpu()\n",
    "    \n",
    "    # Create visualization figure\n",
    "    fig, axes = plt.subplots(num_images, 4, figsize=(16, 4*num_images))\n",
    "    \n",
    "    # Handle single image case\n",
    "    if num_images == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Convert to numpy for plotting\n",
    "        original_np = original_images[i].permute(1, 2, 0).numpy()\n",
    "        mask_np = masks[i, 0].numpy()\n",
    "        composed_np = composed_images[i].permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Create masked visualization (original with mask overlay)\n",
    "        masked_vis = original_images[i].clone()\n",
    "        masked_vis = masked_vis * (1 - masks[i])  # Black out masked regions\n",
    "        masked_vis_np = masked_vis.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Calculate MAE only on masked regions\n",
    "        mask_3ch = masks[i].expand(3, -1, -1)\n",
    "        mae_masked = (torch.abs(predicted_images_denorm[i] - original_images[i]) * mask_3ch).sum() / (mask_3ch.sum() + 1e-8)\n",
    "        \n",
    "        # Plot\n",
    "        axes[i, 0].imshow(original_np)\n",
    "        axes[i, 0].set_title(f'Original\\n{filenames[i]}', fontsize=10, fontweight='bold')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(mask_np, cmap='gray')\n",
    "        axes[i, 1].set_title('Segmentation Mask', fontsize=10, fontweight='bold')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(masked_vis_np)\n",
    "        axes[i, 2].set_title('Masked Input', fontsize=10, fontweight='bold')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        axes[i, 3].imshow(composed_np)\n",
    "        axes[i, 3].set_title(f'Inpainted Result\\nMAE: {mae_masked:.4f}', fontsize=10, fontweight='bold')\n",
    "        axes[i, 3].axis('off')\n",
    "        \n",
    "        # Save individual result\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(composed_np)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(save_dir, f'inpainted_{filenames[i]}')\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'comparison_grid.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Results saved to '{save_dir}/'\")\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN INFERENCE SCRIPT\n",
    "# ============================================================================\n",
    "\n",
    "# Load your trained inpainting model\n",
    "model = LightningModule.load_from_checkpoint(\n",
    "    'denoising_weights/version_1/checkpoints/best_valid_loss.ckpt',\n",
    "    use_weighted_loss=True\n",
    ")\n",
    "\n",
    "# Setup data module for segmentation dataset\n",
    "inpainting_dm = InpaintingInferenceDataModule(\n",
    "    data_dir=\"dataset\", \n",
    "    batch_size=4,\n",
    "    num_workers=4\n",
    ")\n",
    "inpainting_dm.setup()\n",
    "\n",
    "# Run inference on test set\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Running inference on {len(inpainting_dm.test_dataset)} test images...\")\n",
    "\n",
    "visualize_inpainting_segmentation(\n",
    "    model=model,\n",
    "    dataloader=inpainting_dm.test_dataloader(),\n",
    "    device=device,\n",
    "    num_images=8,  # Visualizar 8 ejemplos\n",
    "    save_dir='inpainting_segmentation_results'\n",
    ")\n",
    "\n",
    "# También puedes correr en el training set si quieres\n",
    "print(f\"\\nRunning inference on {len(inpainting_dm.train_dataset)} training images...\")\n",
    "visualize_inpainting_segmentation(\n",
    "    model=model,\n",
    "    dataloader=inpainting_dm.train_dataloader(),\n",
    "    device=device,\n",
    "    num_images=8,\n",
    "    save_dir='inpainting_segmentation_results_train'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
